---
permalink: fas8200/chassis-replace-shutdown.html 
sidebar: sidebar 
keywords: fas8200, shutdown the controllers 
summary: Per sostituire lo chassis, è necessario spegnere i controller. 
---
= Spegnere i controller - FAS8200
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Per sostituire lo chassis, è necessario spegnere i controller.



== Opzione 1: La maggior parte delle configurazioni

Questa procedura è valida solo per configurazioni a 2 nodi non MetroCluster. Se si dispone di un sistema con più di due nodi, vedere https://kb.netapp.com/Advice_and_Troubleshooting/Data_Storage_Software/ONTAP_OS/How_to_perform_a_graceful_shutdown_and_power_up_of_one_HA_pair_in_a_4__node_cluster["Come eseguire uno spegnimento e l'accensione di una coppia ha in un cluster a 4 nodi"^].

.Prima di iniziare
Hai bisogno di:

* Credenziali dell'amministratore locale per ONTAP.
* Passphrase per la gestione delle chiavi integrata di NetApp (OKM) a livello del cluster se si utilizza la crittografia storage o NVE/NAE.
* Accessibilità BMC per ciascun controller.
* Impedire a tutti i client/host di accedere ai dati sul sistema NetApp.
* Sospendere i processi di backup esterni.
* Strumenti e attrezzature necessari per la sostituzione.



NOTE: Se il sistema è un NetApp StorageGRID o ONTAP S3 utilizzato come Tier cloud FabricPool, fare riferimento a. https://kb.netapp.com/onprem/ontap/hardware/What_is_the_procedure_for_graceful_shutdown_and_power_up_of_a_storage_system_during_scheduled_power_outage#["Arrestare e accendere correttamente il sistema storage Guida alla risoluzione dei problemi"] dopo aver eseguito questa procedura.


NOTE: Se si utilizzano SSD, fare riferimento a. https://kb.netapp.com/Support_Bulletins/Customer_Bulletins/SU490["SU490: (Impatto: Critico) Best Practice SSD: Evita il rischio di guasti al disco e perdita di dati se spento per più di due mesi"]

Come Best practice prima dello spegnimento, è necessario:

* Eseguire ulteriori operazioni https://kb.netapp.com/onprem/ontap/os/How_to_perform_a_cluster_health_check_with_a_script_in_ONTAP["controlli dello stato del sistema"].
* Aggiornare ONTAP a una versione consigliata per il sistema.
* Risolvere qualsiasi https://activeiq.netapp.com/["Avvisi e rischi relativi al benessere Active IQ"]. Annotare eventuali guasti presenti nel sistema, ad esempio i LED sui componenti del sistema.


.Fasi
. Accedere al cluster tramite SSH o da qualsiasi nodo del cluster utilizzando un cavo della console locale e un laptop/console.
. Disattivare AutoSupport e indicare per quanto tempo si prevede che il sistema non sia in linea:
+
`system node autosupport invoke -node * -type all -message "MAINT=8h Power Maintenance"`

. Identificare l'indirizzo SP/BMC di tutti i nodi:
+
`system service-processor show -node * -fields address`

. Uscire dalla shell del cluster: `exit`
. Accedere a SP/BMC tramite SSH utilizzando l'indirizzo IP di uno qualsiasi dei nodi elencati nell'output del passaggio precedente.
+
Se si utilizza una console/laptop, accedere al controller utilizzando le stesse credenziali di amministratore del cluster.

+

NOTE: Aprire una sessione SSH per ogni connessione SP/BMC in modo da poter monitorare l'avanzamento.

. Arrestare i 2 nodi situati nel telaio danneggiato:
+
`system node halt -node <node>,<node2> -skip-lif-migration-before-shutdown true -ignore-quorum-warnings true -inhibit-takeover true`

+

NOTE: Per i cluster che utilizzano SnapMirror Synchronous che operano in modalità StrictSync: `system node halt -node <node>,<node2>  -skip-lif-migration-before-shutdown true -ignore-quorum-warnings true -inhibit-takeover true -ignore-strict-sync-warnings true`

. Immettere *y* per ogni controller nel cluster quando viene visualizzato `_Warning: Are you sure you want to halt node "cluster <node-name> number"?
{y|n}:_`
. Attendere che ogni controller si arresti e visualizzi il prompt DEL CARICATORE.




== Opzione 2: Il controller si trova in una configurazione MetroCluster a due nodi

Per spegnere il controller compromesso, è necessario determinare lo stato del controller e, se necessario, sostituirlo in modo che il controller integro continui a servire i dati provenienti dallo storage del controller compromesso.

.A proposito di questa attività
* Al termine di questa procedura, è necessario lasciare accesi gli alimentatori per alimentare il controller integro.


.Fasi
. Controllare lo stato MetroCluster per determinare se il controller compromesso è passato automaticamente al controller integro: `metrocluster show`
. A seconda che si sia verificato uno switchover automatico, procedere come indicato nella seguente tabella:
+
[cols="1,2"]
|===
| Se il controller è compromesso... | Quindi... 


 a| 
Si è attivata automaticamente
 a| 
Passare alla fase successiva.



 a| 
Non si è attivato automaticamente
 a| 
Eseguire un'operazione di switchover pianificata dal controller integro: `metrocluster switchover`



 a| 
Non è stato attivato automaticamente, si è tentato di eseguire lo switchover con `metrocluster switchover` e lo switchover è stato vetoed
 a| 
Esaminare i messaggi di veto e, se possibile, risolvere il problema e riprovare. Se non si riesce a risolvere il problema, contattare il supporto tecnico.

|===
. Risincronizzare gli aggregati di dati eseguendo `metrocluster heal -phase aggregates` dal cluster esistente.
+
[listing]
----
controller_A_1::> metrocluster heal -phase aggregates
[Job 130] Job succeeded: Heal Aggregates is successful.
----
+
Se la riparazione è vetoed, si ha la possibilità di riemettere il `metrocluster heal` con il `-override-vetoes` parametro. Se si utilizza questo parametro opzionale, il sistema sovrascrive qualsiasi veto soft che impedisca l'operazione di riparazione.

. Verificare che l'operazione sia stata completata utilizzando il comando MetroCluster Operation show.
+
[listing]
----
controller_A_1::> metrocluster operation show
    Operation: heal-aggregates
      State: successful
Start Time: 7/25/2016 18:45:55
   End Time: 7/25/2016 18:45:56
     Errors: -
----
. Controllare lo stato degli aggregati utilizzando `storage aggregate show` comando.
+
[listing]
----
controller_A_1::> storage aggregate show
Aggregate     Size Available Used% State   #Vols  Nodes            RAID Status
--------- -------- --------- ----- ------- ------ ---------------- ------------
...
aggr_b2    227.1GB   227.1GB    0% online       0 mcc1-a2          raid_dp, mirrored, normal...
----
. Riparare gli aggregati root utilizzando `metrocluster heal -phase root-aggregates` comando.
+
[listing]
----
mcc1A::> metrocluster heal -phase root-aggregates
[Job 137] Job succeeded: Heal Root Aggregates is successful
----
+
Se la riparazione è vetoed, si ha la possibilità di riemettere il `metrocluster heal` comando con il parametro -override-vetoes. Se si utilizza questo parametro opzionale, il sistema sovrascrive qualsiasi veto soft che impedisca l'operazione di riparazione.

. Verificare che l'operazione di riparazione sia completa utilizzando `metrocluster operation show` sul cluster di destinazione:
+
[listing]
----

mcc1A::> metrocluster operation show
  Operation: heal-root-aggregates
      State: successful
 Start Time: 7/29/2016 20:54:41
   End Time: 7/29/2016 20:54:42
     Errors: -
----
. Sul modulo controller guasto, scollegare gli alimentatori.

